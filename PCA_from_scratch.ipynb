{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"ac209a_hw6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "## Homework 6  AC 209 : PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2022**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "\n",
    "- **PLEASE NOTE:**\n",
    "\n",
    "  - There are no coding exercises in this assignment.\n",
    "  - All of your mathematical statements should be written out in markdown cells using $\\LaTeX$.\n",
    "  - **Be sure to show your work so we can see how you arrived at your solutions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class='exercise'> Question 1 [20 pts] </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose we have a data matrix, $X \\in \\Re^{nÃ—p}$, where $n > p$ and the columns have been set to have zero mean. We then conduct eigendecomposition of $X^T X$ to obtain eigenvalues $\\lambda_1, \\ldots , \\lambda_p$ and eigenvectors $Q$, which are the principle components of the original $X$ matrix, i.e.:\n",
    "\n",
    "$$ X^T X = Q \\Lambda Q ^T $$\n",
    "\n",
    "Now let $Q_m$ be the matrix whose first $m$ columns contain the first $m < p$ principle components and the remaining columns are zeros, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. \n",
    "\n",
    "In this assignment, we want to explore the amount of information loss, or 'reconstruction error,' incurred when we limit our PCA transformation of $X$ to use only the first $m$ principle components. The **squared reconstruction error** is:\n",
    "\n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** Suppose that the matrix norm were simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the squared reconstruction error as a sum of matrix products.\n",
    "\n",
    "**Hints:**\n",
    "    \n",
    "* $(A + B)^T = A^T + B^T$\n",
    "* $(AB)^T = B^TA^T$\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by specifying:\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "$$\\implies  \\parallel XQ-XQ_m \\parallel ^2 = (XQ-XQ_m)^T (XQ-XQ_m) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the hint provided, we have: \n",
    "  \n",
    "$$(XQ-XQ_m)^T$$\n",
    "$$\\Leftrightarrow(XQ)^T - (XQ_m)^T$$\n",
    "$$ \\Rightarrow Q^TX^T - Q_m^TX^T$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we have:  \n",
    "$(XQ-XQ_m)^T (XQ-XQ_m) =  (Q^TX^T - Q_m^TX^T) (XQ-XQ_m)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the fact that Matrix Multiplication has a distributive property. This means that we can reorder scalar multiplication:  \n",
    "  \n",
    "$$(Q^TX^T - Q_m^TX^T) (XQ-XQ_m)$$\n",
    "\n",
    "$$ \\implies Q^TX^TXQ - Q^TX^TXQ_m - Q_m^TX^TXQ + Q_m^TX^TXQ_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:  \n",
    "$X^T X = Q \\Lambda Q ^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore: \n",
    "  \n",
    "$$Q^TX^TXQ - Q^TX^TXQ_m - Q_m^TX^TXQ + Q_m^TX^TXQ_m = Q^TQ \\Lambda Q ^TQ - Q^TQ \\Lambda Q ^TQ_m - Q_m^TQ \\Lambda Q ^TQ + Q_m^TQ \\Lambda Q ^TQ_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2**  Simplify your result from 1.1 based on properties of the matrix $Q$.\n",
    "    \n",
    "**Hint:**\n",
    "    \n",
    "* Recall that eiganvectors are orthogonal\n",
    "* For an orthogonal matrix $Q$:\n",
    "    * $Q^{-1} = Q$\n",
    "    * $Q^TQ = QQ^T = I$\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the second hint above:  \n",
    "  \n",
    "$$Q^TQ \\Lambda Q ^TQ - Q^TQ \\Lambda Q ^TQ_m - Q_m^TQ \\Lambda Q ^TQ + Q_m^TQ \\Lambda Q ^TQ_m $$\n",
    "\n",
    " \n",
    "$$ \\Leftrightarrow I \\Lambda I - I \\Lambda Q ^TQ_m - Q_m^TQ \\Lambda I + Q_m^TQ \\Lambda Q ^TQ_m $$  \n",
    "  \n",
    "  \n",
    "$$\\Leftrightarrow \\Lambda  - \\Lambda Q ^TQ_m - \\Lambda Q_m^TQ   + Q_m^TQ \\Lambda Q ^TQ_m $$ \n",
    "\n",
    "$$\\Leftrightarrow \\Lambda  - \\Lambda Q ^TQ_m - \\Lambda Q_m^TQ   + Q_m^TQ \\Lambda Q ^TQ_m $$\n",
    "\n",
    "\n",
    "$$\\Leftrightarrow \\Lambda  - \\Lambda Q ^TQ_m - \\Lambda Q_m^TQ   + Q_m^TQ \\Lambda Q ^TQ_m $$ \n",
    "  \n",
    "$$ \\implies \\Lambda  - \\Lambda Q ^TQ_m - \\Lambda Q_m^TQ   + Q_m^TQ \\Lambda Q ^TQ_m $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** Recalling the definition of $Q_m$ from earlier, express the products $Q^T_m Q$ and $Q^T Q_m$ in a simplified form.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "* Recall the properties of orthogonal matricies from above\n",
    "* Remember that some of the columns of $Q_m$ are zeros\n",
    "* Your result should be expressed as a single matrix\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q_m = \\begin{bmatrix}q_1, \\ldots, q_m, 0, \\ldots, 0\\end{bmatrix} \\in \\Re^{p \\times p}$   \n",
    "$Q = \\begin{bmatrix}q_1, \\ldots, q_m, q_{m+1} \\ldots, q_p \\end{bmatrix} \\in \\Re^{p \\times p}$\n",
    "\n",
    "$Q^T = \\begin{bmatrix}q_1^T \\\\ \\vdots \\\\ q_m^T \\\\ q_{m+1}^T \\\\ \\vdots \\\\ q_p^T \\end{bmatrix} \\in \\Re^{p \\times p}$   \n",
    "  \n",
    "$Q_m^T = \\begin{bmatrix}q_1^T \\\\ \\vdots \\\\ q_m^T \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\in \\Re^{p \\times p}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Therefore:\n",
    "\n",
    "\n",
    "$Qm^TQ =  \\begin{bmatrix}q_1^T \\\\ \\vdots \\\\ q_m^T \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\begin{bmatrix}q_1, \\ldots, q_m, q_{m+1} \\ldots, q_p \\end{bmatrix} $\n",
    "$ = \\begin{bmatrix} \n",
    "q_1^Tq_1 & q_1^Tq_2 & \\ldots & q_1^Tq_m & q_1^Tq_{m+1} & \\ldots & q_1^Tq_p \\\\\n",
    "q_2^Tq_1 & q_2^Tq_2 & \\ldots & q_2^Tq_m & q_2^Tq_{m+1} & \\ldots & q_2^Tq_p  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "q_m^Tq_1 & q_m^Tq_2 & \\ldots & q_m^Tq_m & q_m^Tq_{m+1} & \\ldots & q_m^Tq_p  \\\\\n",
    "0_{m+1}q_1 & 0_{m+1}q_2 & \\ldots & 0_{m+1}q_m & 0_{m+1}q_{m+1} & \\ldots & 0_{m+1}q_p  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0_{p}q_1 & 0_{p}q_2 & \\ldots & 0_{p}q_m & 0_{p}q_{m+1} & \\ldots & 0_{p}q_p  \\\\\n",
    "\\end{bmatrix}$\n",
    "$= \\begin{bmatrix} \n",
    "1_{1,1}& 0 & \\ldots & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & 1_{2,2} & \\ldots & 0 & 0 & \\ldots & 0  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 1_{m,m} & 0 & \\ldots & 0  \\\\\n",
    "0 & 0 & \\ldots & 0 & 0_{m+1,m+1} & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 0 & 0 & \\ldots & 0_{p,p} \\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly,  \n",
    "\n",
    "$Q^TQ_m = \\begin{bmatrix}q_1^T \\\\ \\vdots \\\\ q_m^T \\\\ q_{m+1}^T \\\\ \\vdots \\\\ q_p^T \\end{bmatrix} \\begin{bmatrix}q_1, \\ldots, q_m, 0, \\ldots, 0\\end{bmatrix}$=$\\begin{bmatrix} \n",
    "q_1^Tq_1 & q_1^Tq_2 & \\ldots & q_1^Tq_m & q_1^T0 & \\ldots & q_1^T0 \\\\\n",
    "q_2^Tq_1 & q_2^Tq_2 & \\ldots & q_2^Tq_m & q_2^T0 & \\ldots & q_2^T0  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "q_m^Tq_1 & q_m^Tq_2 & \\ldots & q_m^Tq_m & q_m^T0 & \\ldots & q_m^T0  \\\\\n",
    "q_{m+1}^Tq_1 & q_{m+1}q_2 & \\ldots & q_{m+1}q_m & q_{m+1}0 & \\ldots & q_{m+1}^T0  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "q_{p}^Tq_1 & q_{p}q_2 & \\ldots & q_{p}q_m & q_{p}0 & \\ldots & q_{p}^T0  \\\\\n",
    "\\end{bmatrix}=$\n",
    "$\\begin{bmatrix} \n",
    "1_{1,1}& 0 & \\ldots & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & 1_{2,2} & \\ldots & 0 & 0 & \\ldots & 0  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 1_{m,m} & 0 & \\ldots & 0  \\\\\n",
    "0 & 0 & \\ldots & 0 & 0_{m+1,m+1} & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 0 & 0 & \\ldots & 0_{p,p} \\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we obtain the matrix $I_m$, where $I_m = Q^T_m Q = Q^T Q_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4**  Use your results from 1.3 to finally fully simplify your expression from 1.2.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Lambda  - \\Lambda Q ^TQ_m - \\Lambda Q_m^TQ   + Q_m^TQ \\Lambda Q ^TQ_m $$\n",
    "  \n",
    "$$ = \\Lambda  - \\Lambda I_m - \\Lambda I_m  + I_m \\Lambda I_m $$\n",
    "\n",
    "$$ = \\Lambda  - \\Lambda I_m - \\Lambda I_m  +\\Lambda I_m $$\n",
    "$$ = \\Lambda  - \\Lambda I_m$$\n",
    "\n",
    "This last matrix is diagonal, meaning that diagonal entries are all zero except the last $m+1$ to $p$ entries which are the *eigenvalues*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Lambda  - \\Lambda I_m$ =\\begin{bmatrix} \n",
    "0_{1,1}& 0 & \\ldots & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0_{2,2} & \\ldots & 0 & 0 & \\ldots & 0  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 0_{m,m} & 0 & \\ldots & 0  \\\\\n",
    "0 & 0 & \\ldots & 0 & \\lambda_{m+1,m+1} & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 0 & 0 & \\ldots & \\lambda_{p,p} \\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.5** Note that the result you obtained in 1.4 is still a matrix. But this *cannot* be the value of a proper norm on the space of matrices (since the value should be a scalar). The *true* matrix norm should actually be the **trace** of the above result, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "\n",
    "Use your result from 1.4 and this new definition of the matrix norm to find a simple expression for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "* The trace of an $n\\times n$ matrix is defined as the sum of its diagonal elements: ${\\rm trace}(A) = \\sum_{i=1}^n a_{ii}$\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convience we denote\n",
    "$\\Lambda  - \\Lambda I_m$ as \n",
    "$\\Lambda_m$\n",
    "\n",
    "$ \\parallel \\Lambda I_m \\parallel ^2  = {\\rm trace} (\\Lambda_m^T \\Lambda_m ) $\n",
    "\n",
    "\n",
    "The transpose of a diagonal matrix is equal to the original matrix. This means that the diagonals of $\\Lambda_m^T \\Lambda_m$ are the squared values of $\\Lambda_m$.\n",
    "\n",
    "\n",
    "$ \\Lambda_m^T \\Lambda_m = \\begin{bmatrix} \n",
    "0_{1,1}& 0 & \\ldots & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0_{2,2} & \\ldots & 0 & 0 & \\ldots & 0  \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 0_{m,m} & 0 & \\ldots & 0  \\\\\n",
    "0 & 0 & \\ldots & 0 & \\lambda_{m+1,m+1}^2 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "0 & 0 & \\ldots & 0 & 0 & \\ldots & \\lambda_{p,p}^2 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This matrix $\\Lambda_m^T \\Lambda_m$ is *square*. This means that its trace is simply the *sum of its diagonal entries*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A)  = \\sum_{i=1}^{p}a_{i,p}^2$\n",
    "\n",
    "Where $a_{i,p} = 0$ for $i=0,1,2,3....m$, and $\\lambda_{i,p}$ for $i=m+1, m+2, ...p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.6** Interpret your result from 1.5. In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results from 1.5 mean that **the squared reconstruction error can be expressed as a sum of our eigenvalues**.  \n",
    "Choosing $m$ substantially large s.t. $m$ approaches $p$ results in our matrix  $\\Lambda_m$  becoming a diagonal matrix with nearly all of zeros on the diagonal. This means that the squared reconstruction error approaches zero as $m$ increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This concludes HW6-209. Thank you!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  },
  "vscode": {
   "interpreter": {
    "hash": "908abd7e78fd4d71ba1be92795635fd82be5080a16e3cc7c1eae8bbfec458fa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
